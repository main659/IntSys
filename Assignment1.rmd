---
title: "Assignment 1"
author: "David Ocepek, Matevž Eržen"
date: "8. 11. 2019"
output: html_document
---

# Task 1

-----

### What does it mean for a function to be non-convex?

*A real-valued function defined on an n-dimensional interval is called non-convex if the line segment between any two points on the graph of the function crosses the graph of the function.*

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)

library(GA)
library(e1071)
library(dplyr)
library(caret)
library(parallel)
library(doParallel)

monitor_1 <- function(obj) 
{
  index <- match(max(obj@fitness), obj@fitness)
  point <- obj@population[index,1:2]
  points(trans3d(point[1],point[2],f(point[1],point[2]),p1mat), col="darkred", pch=4)
}

monitor_2 <- function(obj) 
{
  index <- match(max(obj@fitness), obj@fitness)
  point <- obj@population[index,1:2]
  points(trans3d(point[1],point[2],f(point[1],point[2]),p2mat), col="darkred", pch=4)
}

monitor_3 <- function(obj) 
{
  index <- match(max(obj@fitness), obj@fitness)
  point <- obj@population[index,1:2]
  points(trans3d(point[1],point[2],f(point[1],point[2]),p3mat), col="darkred", pch=4)
}

monitor_4 <- function(obj) 
{
  index <- match(max(obj@fitness), obj@fitness)
  point <- obj@population[index,1:2]
  points(trans3d(point[1],point[2],f(point[1],point[2]),p4mat), col="darkred", pch=4)
}

monitor_5 <- function(obj) 
{
  index <- match(max(obj@fitness), obj@fitness)
  point <- obj@population[index,1:2]
  points(trans3d(point[1],point[2],f(point[1],point[2]),p5mat), col="darkred", pch=4)
}

par(mfrow=c(1, 4), mar=c(1,1,1,1))
```

```{r rosenbrock}

f <- function(x, y)
{
  z <- (1 - x)^2 + exp(1) * (y - x^2)^2
  
  z
}

x <- seq(-1,1, length=20)
y <- x
z <- outer(x,y,f)

pmat <- persp(x,y,z, col="lightblue", theta=90)


p1mat <- persp(x,y,z, col="lightblue", theta=110, phi=5, main="crossover=blx, pmutation=0.1")
GA1 <- ga(type = "real-valued", fitness = function(x) f(x[1], x[2]), monitor = monitor_1, maxiter = 100, popSize = 50, lower = c(-1, -1), upper = c(1, 1), crossover = gareal_blxCrossover, pmutation = 0.1)
points(trans3d(GA1@solution[1],GA1@solution[2],f(GA1@solution[1],GA1@solution[2]),p1mat), col="red", pch=21)

p2mat <- persp(x,y,z, col="lightblue", theta=110, phi=5, main="crossover=blx, pmutation=0.5")
GA2 <- ga(type = "real-valued", fitness = function(x) f(x[1], x[2]), monitor = monitor_2, maxiter = 100, popSize = 50, lower = c(-1, -1), upper = c(1, 1), crossover = gareal_blxCrossover, pmutation = 0.5)
points(trans3d(GA2@solution[1],GA2@solution[2],f(GA2@solution[1],GA2@solution[2]),p2mat), col="red", pch=21)

p3mat <- persp(x,y,z, col="lightblue", theta=110, phi=5, main="crossover=laplace, pmutation=0.1")
GA3 <- ga(type = "real-valued", fitness = function(x) f(x[1], x[2]), monitor = monitor_3, maxiter = 100, popSize = 50, lower = c(-1, -1), upper = c(1, 1), crossover = gareal_laplaceCrossover, pmutation = 0.1)
points(trans3d(GA3@solution[1],GA3@solution[2],f(GA3@solution[1],GA3@solution[2]),p3mat), col="red", pch=21)

p4mat <- persp(x,y,z, col="lightblue", theta=110, phi=5, main="crossover=laplace, pmutation=0.5")
GA4 <- ga(type = "real-valued", fitness = function(x) f(x[1], x[2]), monitor = monitor_4, maxiter = 100, popSize = 50, lower = c(-1, -1), upper = c(1, 1), crossover = gareal_laplaceCrossover, pmutation = 0.5)
points(trans3d(GA4@solution[1],GA4@solution[2],f(GA4@solution[1],GA4@solution[2]),p4mat), col="red", pch=21)

```

### How does performance vary when you are increasing the number of iterations?

*Function takes exponentially more time, but doesn't produce any better results (if we are talking about >100 iterations in our case).*

### What about population size?

*Population size doesn't cause as great difference in execution time as increase in number of iterations does, but it's still noticable. At the same time it doesn't make a difference if we increase the population size over certain number (in this case 10-20).*

*Both previous answers are based on results of testing on this exact problem.*

### Explain the difference between local and global maxima.

*Local maximum is the greatest element in a subset or a given range of a function. Global maximum is the greatest value among all elements in a set or values of a function. There can be multiple local maximas, while there can be only one global maximum.*


# Task 2

----

```{r feature selection}

data <- read.csv("C:\\Users\\Matevz\\Desktop\\DLBCL.csv", header=TRUE, sep=",")
data$X = NULL
data$atclass = NULL

trainIndex <- createDataPartition(data$class, p = .8, list = FALSE, times = 1)
dataTrain <- data[ trainIndex,]
dataTest  <- data[-trainIndex,]

ctrl <- trainControl(
  number = 3,
  method="cv"
)

treeModel <- function(binSelection)
{
  columns <- seq(1, length(binSelection))
  selectedCol <- columns[as.logical(binSelection)]
  selectedCol <- append(selectedCol, length(binSelection) + 1, after = length(selectedCol))
  
  training <- select(dataTrain, selectedCol)
  
  plsFit <- train(
    class ~ .,
    data = training,
    method = "ctree",
    trControl = ctrl
  )
  
  return(plsFit)
}

fitnessFunc <- function(binSelection)
{
  if (sum(binSelection) < 2 || sum(binSelection) > 1000)
  {
    return(-1)
  }
  else 
  {
    plsFit <- treeModel(binSelection)
    
    return(mean(plsFit$results$Accuracy)/sum(binSelection))
  }
}

f <- f <- function(x, y)
{
  z <- x/y
  
  z
}

x <- seq(0,1, length=100)
y <- seq(3,999, length=100)
z <- outer(x,y,f)

p5mat <- persp(x,y,z, col="lightblue", theta=110, phi=5, main="crossover=gabin_uCrossover, pmutation=0.2")

GA <- ga(
  type = "binary", 
  fitness = fitnessFunc,
  monitor = monitor_5,
  crossover = gabin_uCrossover, 
  elitism = 2, 
  pmutation = 0.2,
  nBits = 1070, 
  maxiter = 100,
  run = 10, 
  popSize = 40, 
  parallel = T, 
  keepBest = TRUE)

summary(GA)

```

### Was the feature selection successful?

*Feature selection was succesful. We managed to reduce the number of features by x0.57, reducing the original set of 1070 features to ~460 (varies) with no significant accuracy loss.*

### The procedure might overfit the data set. How would you prevent it?

*Overfitting is a natural and unavoidable process while training a model and the general solution is splitting the data into a training and test set. The dataset was split 80% training set and 20% test set.*

```{r results}
# Number of features
sum(GA@solution[1,])

# Accuracy of test set and everything combined
finalFit <- treeModel(GA@solution[1,])
predicted <- predict(finalFit, dataTest)
confusionMatrix(predicted, dataTest$class)
predicted <- predict(finalFit, data)
confusionMatrix(predicted, data$class)

```

### Suggest how to improve the selection process.

*There is always room for improvement in terms of creating better fitness function and tweaking parameters of GA. In our case this is very much based on experimentation.*

### What are some of the key properties of the fitness function?

*Some of the key properties of the fitness function are negative values for to many or two few features, binarity of input, convexness within boundaries, ...*

### Compare the final result (set of features) with the same number of features, that correlate the most with the target variable. What do you observe?

*We were able to achieve very high percent of the features with the highest correlation - maybe even to high. This might indicate the overfitting.*





