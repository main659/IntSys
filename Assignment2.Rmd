---
title: "Assignment2"
author: "David Ocepek, Matevz Erzen"
date: "1/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if (!require("tm")) install.packages("tm")
if (!require("wordcloud")) install.packages("wordcloud")
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("NLP")) install.packages("NLP")
if (!require("openNLP")) install.packages("openNLP")
```

```{r libraries}
library(tm)
library(wordcloud)
library(ggplot2)
library(NLP)
library(openNLP)
```


# Cleaning

-----

```{r cleanning}
train = read.table(file = "C:\\Users\\david\\OneDrive\\Dokumente\\GitHub\\IntelligentSystems\\insults\\train.tsv",
                   sep = '\t', 
                   header = TRUE,
                   stringsAsFactors=FALSE 
                   )
test = read.table(file = "C:\\Users\\david\\OneDrive\\Dokumente\\GitHub\\IntelligentSystems\\insults\\test.tsv",
                  sep = '\t', 
                  header = TRUE,
                  stringsAsFactors=FALSE 
                  )

handle_special_char <- function(doc)
{
  doc = gsub("\\\\x[0-9a-fA-F]{2}", " ", doc) #UTF-8 characters
  doc = gsub("\\x[0-9a-fA-F]{2}", " ", doc) #UTF-8 characters
  doc = gsub("\\[0-9a-fA-F]", " ", doc) #non-alfanumeric chars
  doc = iconv(doc, "latin1", "ASCII", sub="")
  
  return(doc)
}

conn = file("english.stop.txt", open="r")
mystopwords = readLines(conn)
close(conn)

y = train[,1]
X = train[,2]
X <- handle_special_char(X) #Remove special utf8 characters
X <- Corpus(VectorSource(X))
#TO-DO: Replace proper nouns
X <- tm_map(X, content_transformer(tolower)) #Transform to lower
X <- tm_map(X, removePunctuation) #Remove punctuations
X <- tm_map(X, removeNumbers) #Remove number
X <- tm_map(X, removeWords, stopwords("english")) #Remove stopwords
X <- tm_map(X, removeWords, mystopwords) #Remove custom stopwords
X <- tm_map(X, stemDocument) #Get radicals
X <- tm_map(X, stripWhitespace) #Have only one white space
```

# Exploration

-----

```{r frequency analisys}
tdm <- TermDocumentMatrix(X)
termFrequency <- rowSums(as.matrix(tdm))
qplot(seq(length(termFrequency)),sort(termFrequency), xlab = "index", ylab = "Freq")
findFreqTerms(tdm, lowfreq=100)
termFrequency <- subset(termFrequency, termFrequency >= 100)
qplot(seq(length(termFrequency)),sort(termFrequency), xlab = "index", ylab = "Freq")
```

```{r exploration}
dtm <- DocumentTermMatrix(X, control = list(minDocFreq=2, minWordLength=3, normalize = TRUE, weighting=weightTfIdf))
rowTotals <- slam::row_sums(dtm)
dtm <- dtm[rowTotals > 0, ]
mat <- as.matrix(dtm)

pca <- prcomp(mat, scale. = TRUE, rank. = 10)
comp1 <- as.numeric(pca$x[,1])
comp2 <- as.numeric(pca$x[,2])

kmeansResult <- kmeans(mat, 2)
qplot(comp1, comp2, color=kmeansResult$cluster)
kmeansResult <- kmeans(mat, 4)
qplot(comp1, comp2, color=kmeansResult$cluster)
kmeansResult <- kmeans(mat, 8)
qplot(comp1, comp2, color=kmeansResult$cluster)
kmeansResult <- kmeans(mat, 16)
qplot(comp1, comp2, color=kmeansResult$cluster)
qplot(comp1, comp2, color=y[rowTotals > 0])
```
```{r cleanning}
train = read.table(file = "C:\\Users\\david\\OneDrive\\Dokumente\\GitHub\\IntelligentSystems\\insults\\train.tsv",
                   sep = '\t',
                   quote = "",
                   header = TRUE
                   )

handle_special_char <- function(doc)
{
  doc = iconv(doc, "latin1", "ASCII", sub="")
  doc = gsub("\\\\x[0-9a-fA-F]{2}", " ", doc) #UTF-8 characters
  doc = gsub("\\x[0-9a-fA-F]{2}", " ", doc) #UTF-8 characters
  doc = gsub("\\[0-9a-fA-F]", " ", doc) #non-alfanumeric chars
  
  return(doc)
}

conn = file("english.stop.txt", open="r")
mystopwords = readLines(conn)
close(conn)

X = train[,2]
X <- handle_special_char(X) #Remove special utf8 characters
X <- Corpus(VectorSource(X))
```


```{r POS tagging}
#detach("package:ggplot2", unload=TRUE)

train = read.table(file = "C:\\Users\\david\\OneDrive\\Dokumente\\GitHub\\IntelligentSystems\\insults\\train.tsv",
                   sep = '\t', 
                   header = TRUE,
                   stringsAsFactors=FALSE 
                   )

X = train[,2]
X <- handle_special_char(X) #Remove special utf8 characters
X = gsub("\\\\", " ", X) #not the best way to remove slashes

sent_ann <- Maxent_Sent_Token_Annotator()
word_ann <- Maxent_Word_Token_Annotator()
pos_ann <- Maxent_POS_Tag_Annotator()

doctags <- vector()

for (i in 1:length(X))
{
  s <- as.String(X[i])
  
  a1 <- annotate(s, sent_ann)
  a2 <- annotate(s, word_ann, a1)
  a2w <- subset(a2, type == "word")
  a3 <- annotate(s, pos_ann, a2)
  a3w <- subset(a3, type == "word")
  
  tags <- vector()
  for (i in 1:length(a3$features))
    tags <- c(tags, a3$features[[i]]$POS)
  
  table(tags)
  
  tokenPOS <- cbind(s[a3w], tags)
  #TO-DO: join into table
}
```

# Modeling

-----

```{r modeling}
tbl <- table(y)
tbl
tbl[2]/sum(tbl)
```


*The class is mildly biased towards label zero, meaning towards documents that are not insults. This is logical because we can assume that most of online chats are not intended to insult someone.*