---
title: "Assignment2"
author: "David Ocepek, Matevz Erzen"
date: "1/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if (!require("tm")) install.packages("tm")
if (!require("wordcloud")) install.packages("wordcloud")
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("NLP")) install.packages("NLP")
if (!require("openNLP")) install.packages("openNLP")
if (!require("stringi")) install.packages("stringi")
```

```{r libraries}
library(tm)
library(wordcloud)
library(ggplot2)
library(NLP)
library(openNLP)
```


# Cleaning

-----

```{r cleanning}
train_dir <- "C:\\Users\\david\\OneDrive\\Dokumente\\GitHub\\IntelligentSystems\\insults\\train.tsv"
test_dir <- "C:\\Users\\david\\OneDrive\\Dokumente\\GitHub\\IntelligentSystems\\insults\\test.tsv"

train = read.table(file = train_dir, sep = '\t', header = TRUE, stringsAsFactors=FALSE)
test = read.table(file = test_dir, sep = '\t', header = TRUE, stringsAsFactors=FALSE)

handle_special_char <- function(doc)
{
  doc = iconv(doc, "ASCII", "ASCII", sub="") #make sure there are no invalid chars
                                             #checked with stri_enc_mark: All are ASCII
  doc = gsub("\\\\\\\\", "\\\\", doc) #remove backslash padding
  doc = gsub("\\\\xa0", " ", doc) #handle non-carriage return line break
  doc = gsub("\\\\xc2", "", doc) #handle garbage character
  doc = gsub("\\\\x[0-9a-fA-F]{2}", " ", doc) #handle other UTF-8 characters
  
  doc = gsub("\\\\\'", "'", doc) #handle single parenthesys
  doc = gsub("\\\\\"", "\"", doc) #handle double parenthesys
  doc = gsub("\\\\n", " ", doc) #handle new line
  doc = gsub("\\\\t", " ", doc) #handle tab
  doc = gsub("\\\\[0-9a-fA-F]", " ", doc) #handle other special characters
  
  return(doc)
}

conn = file("english.stop.txt", open="r")
mystopwords = readLines(conn)
close(conn)

y = train[,1]
X = train[,2]
X <- handle_special_char(X) #Remove special utf8 characters
X <- Corpus(VectorSource(X))
#TO-DO: Replace proper nouns
X <- tm_map(X, content_transformer(tolower)) #Transform to lower
X <- tm_map(X, removePunctuation) #Remove punctuations
X <- tm_map(X, removeNumbers) #Remove number
X <- tm_map(X, removeWords, stopwords("english")) #Remove stopwords
X <- tm_map(X, removeWords, mystopwords) #Remove custom stopwords
X <- tm_map(X, stemDocument) #Get radicals
X <- tm_map(X, stripWhitespace) #Have only one white space
```

# Exploration

-----

```{r frequency analisys}
tdm <- TermDocumentMatrix(X)
termFrequency <- rowSums(as.matrix(tdm))
qplot(seq(length(termFrequency)),sort(termFrequency), xlab = "index", ylab = "Freq")
findFreqTerms(tdm, lowfreq=100)
termFrequency <- subset(termFrequency, termFrequency >= 100)
qplot(seq(length(termFrequency)),sort(termFrequency), xlab = "index", ylab = "Freq")
```

```{r exploration}
dtm <- DocumentTermMatrix(X, control = list(minDocFreq=2, minWordLength=3, normalize = TRUE, weighting=weightTfIdf))
rowTotals <- slam::row_sums(dtm)
dtm <- dtm[rowTotals > 0, ]
mat <- as.matrix(dtm)

pca <- prcomp(mat, scale. = TRUE, rank. = 2)
comp1 <- as.numeric(pca$x[,1])
comp2 <- as.numeric(pca$x[,2])

kmeansResult <- kmeans(mat, 2)
qplot(comp1, comp2, color=kmeansResult$cluster)
kmeansResult <- kmeans(mat, 4)
qplot(comp1, comp2, color=kmeansResult$cluster)
kmeansResult <- kmeans(mat, 8)
qplot(comp1, comp2, color=kmeansResult$cluster)
kmeansResult <- kmeans(mat, 16)
qplot(comp1, comp2, color=kmeansResult$cluster)
qplot(comp1, comp2, color=y[rowTotals > 0])

#TO-DO: word2vec
```

```{r POS tagging}
detach("package:caret", unload=TRUE)
detach("package:ggplot2", unload=TRUE)
if("ggplot2" %in% (.packages())) detach("package:ggplot2", unload=TRUE)

X = train[,2]
X <- handle_special_char(X) #Remove special utf8 characters

sent_ann <- Maxent_Sent_Token_Annotator()
word_ann <- Maxent_Word_Token_Annotator()
pos_ann <- Maxent_POS_Tag_Annotator()

doctags <- vector("list", length(X))

for (i in 1:length(X))
{
  s <- as.String(X[i])
  
  a1 <- annotate(s, sent_ann)
  a2 <- annotate(s, word_ann, a1)
  a2w <- subset(a2, type == "word")
  a3 <- annotate(s, pos_ann, a2)
  a3w <- subset(a3, type == "word")
  
  tags <- vector()
  for (j in 1:length(a3$features))
    tags <- c(tags, a3$features[[j]]$POS)
  
  table(tags)
  
  tokenPOS <- cbind(s[a3w], tags)
  doctags[[i]] <- s[a3w]
}

#TO-DO: Meaningfully demonstrate POS tagging
```


# Modeling

-----

```{r modeling}
tbl <- table(y)
tbl
tbl[2]/sum(tbl)
```


*The class is mildly biased towards label zero, meaning towards documents that are not insults. This is logical because we can assume that most of online chats are not intended to insult someone.*